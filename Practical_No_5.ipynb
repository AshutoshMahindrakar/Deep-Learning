{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIPl9n136ZL1",
        "outputId": "51093406-db8f-448a-8281-c9275357db86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 556ms/step - accuracy: 0.6714 - loss: 0.5991 - val_accuracy: 0.8300 - val_loss: 0.3887\n",
            "Epoch 2/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 499ms/step - accuracy: 0.8685 - loss: 0.3265 - val_accuracy: 0.8370 - val_loss: 0.3827\n",
            "Epoch 3/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 493ms/step - accuracy: 0.8788 - loss: 0.2990 - val_accuracy: 0.8362 - val_loss: 0.3766\n",
            "Epoch 4/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 494ms/step - accuracy: 0.9057 - loss: 0.2456 - val_accuracy: 0.8426 - val_loss: 0.3796\n",
            "Epoch 5/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 507ms/step - accuracy: 0.9233 - loss: 0.2016 - val_accuracy: 0.8482 - val_loss: 0.4089\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step\n",
            "Test Accuracy: 0.8488\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85     12500\n",
            "           1       0.85      0.84      0.85     12500\n",
            "\n",
            "    accuracy                           0.85     25000\n",
            "   macro avg       0.85      0.85      0.85     25000\n",
            "weighted avg       0.85      0.85      0.85     25000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Practical No 5\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the IMDB movie review dataset\n",
        "num_words = 10000  # Consider only the top 10,000 most frequent words\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "\n",
        "# Preprocess the data\n",
        "max_len = 200  # Maximum sequence length\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "\n",
        "# Define the neural network architecture\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=num_words, output_dim=128, input_length=max_len),\n",
        "    LSTM(units=64, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(units=1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=5, batch_size=128, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(\"int32\")  # Convert probabilities to binary predictions\n",
        "\n",
        "# Calculate accuracy and print classification report\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    }
  ]
}